[["index.html", "Bibliografie 1 Portfolio", " Bibliografie Wouter 2023-06-30 1 Portfolio Hallo! ik ben Wouter Verra, een student biologisch en medisch laboratorium onderzoek met een minor in bioinformatica. Deze site bevat mijn cv en een aantal voorbeelden van wat ik kan qua coderen. Als er verdere vragen zijn, stuur een mail naar: - "],["cv-wouter-verra.html", "2 CV Wouter Verra", " 2 CV Wouter Verra Algemene informatie Naam: Wouter Verra Adres: - Telefoonnummer: - E-mailadres: - Geboortedatum: 10 April 1996 Geslacht: Man Nationaliteit: Nederlandse Burgerlijke staat: Ongehuwd Opleidingen: Biologisch en Medisch Laboratorium Onderzoek te Utrecht, september 2018 tot heden Keltische Talen en Cultuur te Utrecht, september 2016 tot mei 2016 University Foundation Year (afgerond) Education First te Oxford, September 2015 tot mei 2016 VWO (afgerond) Marecollege te Leiden, September 2007 tot juni 2015 Werkervaring: Hadro techniek, assemblage medewerker, september 2017 tot augustus 2018 Van Vliet The Candy Company, orderpicker, juni 2017 tot september 2017 Mc Donalds Bodegraven, crewlid, mei 2016 tot juni 2017 Vaardigheden: Programeertalen: R Bash Programmas: Rstudio Excel SPSS Talen: Nederlands (moedertaal) Engels (vloeiend) Hobbys: Roeien Boulderen "],["project-noldus.html", "3 Project Noldus", " 3 Project Noldus Samen met twee anderen heb ik een project uitgevoerd voor het bedrijf Noldus About Noldus - Innovative Solutions Noldus (n.d.). Dit is een bedrijf dat zich specialiseert in het maken van producten voor gedragsonderzoek. Een paar voorbeelden hiervan zijn de UltraVox XT Rodent Ultrasonic Vocalizations UltraVox XT (n.d.) welke de geluiden die knaagdieren maken analyseert, FaceReader Facial Expression Recognition Software FaceReader (n.d.) welke menselijke gezichtsuitdrukkingen analyseert en de Erasmus ladder Motor Performance and Motor Learning ErasmusLadder (n.d.). De laatste was de focus van dit onderzoek. De Erasmus ladder bestaat uit twee hokken met een horizontale ladder er tussen. De muizen worden overgehaald uit het hokje te gaan door een luchtstroom en lichtjes waarna het leerd dat als het naar de overkant loopt de wind en lichtjes stoppen, het wil dus zo snel mogelijk naar de overkant. De tred waarmee de muis loopt zegt iets over hoe goed de motor functie is van de muis en daarmee kan gekwantificeerd worden hoe erg de muis is beinvloed door de experimentele conditie Motor Function &amp; Gait Analysis Mouse Models (n.d.). De ladder heeft treden waar een knop onder zit waardoor er gemeten kan worden wat voor soort stap er gezet is en voor hoe lang. Er zijn 4 soorten stappen die gezet kunnen worden, namelijk: korte stap, lange stap, sprong en terug stap en deze informatie wordt allemaal per experiment opgeslagen inclusief de volgorde van de soort stappen per muis. Dit wordt opgeslagen in een grote Excel file met nog veel meer variabelen en metadata per muis zoals leeftijd en sekse. Deze data wordt ingeladen in R, waarna er veel verschillende grafieken mee gemaakt konden worden. Noldus wou graag hun data visualisatie verbeteren, dit werd toen der tijd gedaan met een wat verouderde app waar weinig in aangepast kon worden. De vraag was of wij hier een applicatie voor konden maken welke interactief is. Dit is gedaan door eerst een aantal grafieken en tabellen te maken in aparte R scripts How to Detect Colors in OpenCV [Python] (n.d.). Dit zijn uiteindelijk 11 verschillende tabellen en grafieken geworden welke allemaal naar een ander onderdeel van de data kijken of het op een andere manier visualiseren. Deze scripts zijn hierna aangeroepen in een Shiny app. Shiny Shiny (n.d.) is een open source R package waarmee een applicatie gemaakt kan worden welke redelijk eenvoudig online gedeeld kan worden. De applicatie is te vinden op: Noldus V1.0 (n.d.) Hieronder geef ik een aantal screenshots van de werking van de Noldus applicatie: Als eerste kan de gewenste data ingeladen worden. Daarna is er een tab met algemene data, hieronder valt een tabel met wat metadata, een bargraph met de verdeling van mannelijke en vrouwelijke muizen in het experiment en een tabel met missende data. Vervolgens is er een tab met een interactieve grafiek met verschillende tijd gerelateerde grafieken per trial en een violin plot van de tijd per trial tussen de controle groep en de mutant, deze grafieken zijn interactief doordat de spreiding van de sessies zelf gekozen kan worden waardoor bijvoorbeeld alleen naar sessie 5-7 gekeken kan worden en doordat de tijd gerelateerde grafiek variabele gekozen kan worden. De volgende tab bevat een interactieve grafiek die de gemiddelde hoeveelheid percentage van de stap typen kan laten zien met dezelfde soort sessie spreiding als de vorige grafiek. Daarna is er een staaf diagram die het verschil tussen de controle groep en de mutant kan laten zien in de hoeveelheid percentage van de stappen per staptype en nogmaals de sessie spreiding. Als laatste is er een grafiek die de hoeveelheid percentage stappen visualiseerd als een viool grafiek met als interactie de keuze van staptype en de spreiding van de sessies. "],["transparantie-peer-review.html", "4 Transparantie peer review", " 4 Transparantie peer review Transparantie is een belangrijk onderdeel van onderzoek, het zorgt er voor dat onderzoeken makkelijker te controleren en doorgronden zijn. Hiervoor zijn een aantal criteria opgezet Sumner et al. (2020) , zoals te lezen in de tabel hieronder. Ik ga hierbij twee onderzoeken beoordelen op deze criteria om te kijken hoe transparant ze zijn. Het eerste onderzoek waar ik dit bij ga doen is: Liu et al. (2023) Het is een onderzoek naar het effect van radon op het darm microbioom. Het eerste transparantie criterium is zeker behaald, het doel van het onderzoek is duidelijk omschreven in de laatste paar zinnen van de introductie. De data is volgens het data statement aan het einde van het paper te verkrijgen per aanvraag, Dit maakt het snel reproduceren van het onderzoek niet heel handig. hierdoor haalt het het criterium data availability wel maar data location niet. In de materiaal en methode staat waar de muizen verkregen zijn maar niet waar ze gehouden zijn of waar het onderzoek was dus de study location is grotendeels onbekend. Het author review criterium is wel behaald, de auteurs zijn bekend en waar ze werken. Er is geen aparte ethics statement maar er is in de materiaal en methode te lezen dat het onderzoek is geaccepteerd door the Animal Ethical and Welfare Committee of Academy of Military Medical Sciences (IACUC-AMMS, Beijing, China). De funding statement is aanwezig maar niet heel uitgebreid. Er wordt geen code geleverd bij dit onderzoek dus het code availability criterium is niet behaald. In conclusie, Dit onderzoek is goed geschreven maar niet heel erg transparant. Het moeten aanvragen van de data en het missen van de code maakt reproduceren van dit onderzoek erg ingewikkeld. Voor het tweede onderzoek dat ik ga beoordelen met deze criteria is: Dudel et al. (2020) Het is een onderzoek dat kijkt naar de case-fatality rate van covid-19 tussen verschillende landen. Het study purpose criterium is wel behaald, maar maar net want het was niet makkelijk te vinden wat men wilt met dit onderzoek. De data availability en data location criteria zijn wel behaald aangezien de data vrij te downloaden is vanuit een link in het paper. De data location staat direct in de metadata van het onderzoek dus deze is ook behaald. Het author review criterium is behaald aangezien er vermeld wordt wie het geschreven heeft en zelfs welke onderdelen ze gedaan hebben. De ethics statement mist maar de funding statement is wel aanwezig dus dat criterium wordt wel gehaald. Als laatste is het criterium voor code availability ook behaald, bij de link om de data te downloaden is ook de code die gebruikt is aanwezig. Om te laten zien hoe handig het is om transparant te werken Zal ik een deel van de code van het tweede onderzoek zelf uitvoeren om te kijken of het daadwerkelijk werkt, zie de code met het resultaat hieronder. ### Monitoring trends and differences in COVID-19 case fatality ############## ### rates using decomposition methods: A demographic perspective ############## ### Last updated: 2020-07-22 11:18:52 CEST ### Contact: ### riffe@demogr.mpg.de ### acosta@demogr.mpg.de ### dudel@demogr.mpg.de ### Packages ################################################################## library(tidyverse) library(ggrepel) library(scales) ### Load data ################################################################# # Load data db_gh &lt;- read_csv(here(&quot;assignment_1.2&quot;, &quot;inputdata.csv&quot;)) ### de locatie voor het inlezen van de code moest veranderd worden ### Aggregate data ############################################################ # Filter date db_gh$Date &lt;- as.Date(db_gh$Date,&quot;%d.%m.%y&quot;) db_gh2 &lt;- db_gh %&gt;% filter(Date&lt;=as.Date(&quot;30.06.2020&quot;,&quot;%d.%m.%y&quot;)) # Set New York as &quot;country&quot; (easier handling) db_gh2$Country[db_gh2$Country==&quot;USA&quot; &amp; db_gh2$Region == &quot;NYC&quot;] &lt;- &quot;NYC&quot; # Sum data over age groups db_gh2 &lt;- db_gh2 %&gt;% filter(!Country %in% c(&quot;China&quot;,&quot;USA&quot;,&quot;South Korea&quot;) &amp; Sex == &quot;b&quot;) %&gt;% group_by(Country, Code,Date) %&gt;% summarise(Cases = sum(Cases), Deaths = sum(Deaths)) # Exclude bolletino db_gh2 &lt;- db_gh2 %&gt;% filter(str_sub(Code, 1, 5) != &quot;ITbol&quot;) # Sort by date db_gh2 &lt;- db_gh2 %&gt;% group_by(Country) %&gt;% arrange(Date) # Smooth reporting issues cases for(country in unique(db_gh2$Country)) { days &lt;- db_gh2$Date[db_gh2$Country==country] for(day in 2:length(days)) { current &lt;- db_gh2$Cases[db_gh2$Country==country &amp; db_gh2$Date==days[day]] previous &lt;- db_gh2$Cases[db_gh2$Country==country &amp; db_gh2$Date==days[day-1]] if(current&lt;previous) db_gh2$Cases[db_gh2$Country==country &amp; db_gh2$Date==days[day]] &lt;- previous } } # Smooth reporting issues deaths for(country in unique(db_gh2$Country)) { days &lt;- db_gh2$Date[db_gh2$Country==country] for(day in 2:length(days)) { current &lt;- db_gh2$Deaths[db_gh2$Country==country &amp; db_gh2$Date==days[day]] previous &lt;- db_gh2$Deaths[db_gh2$Country==country &amp; db_gh2$Date==days[day-1]] if(current&lt;previous) db_gh2$Deaths[db_gh2$Country==country &amp; db_gh2$Date==days[day]] &lt;- previous } } ### Plot settings ############################################################# # Set colors col_country &lt;- c(&quot;Germany&quot; = &quot;black&quot;, &quot;Italy&quot; = &quot;#2ca25f&quot;, &quot;NYC&quot;=&quot;#f0027f&quot;, &quot;Spain&quot;=&quot;#beaed4&quot;, &quot;South Korea&quot;=&quot;#fdc086&quot;)#, #&quot;USA&quot;=&quot;#386cb0&quot;) cols &lt;- c(&quot;black&quot;, &quot;#2ca25f&quot;, &quot;#f0027f&quot;, &quot;#beaed4&quot;, &quot;#fdc086&quot;)#, #&quot;#386cb0&quot;) # Axis labs &lt;- db_gh2 %&gt;% group_by(Country) %&gt;% filter(Cases == max(Cases)) %&gt;% mutate(Cases = Cases + 3000) # Including all reports tx &lt;- 6 lim_x &lt;- 240000 ### Plot ###################################################################### db_gh2 %&gt;% ggplot(aes(Cases, Deaths, col = Country))+ geom_line(size = 1, alpha = .9)+ scale_x_continuous(expand = c(0,0), breaks = seq(0, 300000, 50000), limits = c(0, lim_x + 30000), labels = comma)+ scale_y_continuous(expand = c(0,0), breaks = seq(0, 40000, 5000), limits = c(0, 40000), labels = comma)+ annotate(&quot;segment&quot;, x = 0, y = 0, xend = lim_x, yend = lim_x * .02, colour = &quot;grey40&quot;, size = .5, alpha = .3, linetype = 2)+ annotate(&quot;segment&quot;, x = 0, y = 0, xend = lim_x, yend = lim_x * .05, colour = &quot;grey40&quot;, size = .5, alpha = .3, linetype = 2)+ annotate(&quot;segment&quot;, x = 0, y = 0, xend = lim_x, yend = lim_x * .10, colour = &quot;grey40&quot;, size = .5, alpha = .3, linetype = 2)+ annotate(&quot;segment&quot;, x = 0, y = 0, xend = lim_x, yend = lim_x * .15, colour = &quot;grey40&quot;, size = .5, alpha = .3, linetype = 2)+ annotate(&quot;text&quot;, label = &quot;2% CFR&quot;, x = lim_x + 1000, y = lim_x * .02, color=&quot;grey30&quot;, size = tx * .3, alpha = .6, hjust = 0, lineheight = .8) + annotate(&quot;text&quot;, label = &quot;5% CFR&quot;, x = lim_x + 1000, y = lim_x * .05, color=&quot;grey30&quot;, size = tx * .3, alpha = .6, hjust = 0, lineheight = .8) + annotate(&quot;text&quot;, label = &quot;10% CFR&quot;, x = lim_x + 1000, y = lim_x * .10, color=&quot;grey30&quot;, size = tx * .3, alpha = .6, hjust = 0, lineheight = .8) + annotate(&quot;text&quot;, label = &quot;15% CFR&quot;, x = lim_x + 1000, y = lim_x * .15, color=&quot;grey30&quot;, size = tx * .3, alpha = .6, hjust = 0, lineheight = .8) + scale_colour_manual(values = cols)+ geom_text(data = labs, aes(Cases, Deaths, label = Country), size = tx * .35, hjust = 0, fontface = &quot;bold&quot;) + theme_classic()+ labs(x = &quot;Cases&quot;, y = &quot;Deaths&quot;)+ theme( panel.grid.minor = element_blank(), legend.position = &quot;none&quot;, plot.margin = margin(5,5,5,5,&quot;mm&quot;), axis.text.x = element_text(size = tx), axis.text.y = element_text(size = tx), axis.title.x = element_text(size = tx + 1), axis.title.y = element_text(size = tx + 1) ) # Save # ggsave(&quot;Output/Fig_1.jpg&quot;, width = 4, height = 3, dpi = 600) ### ik heb de ggsave gecomment zodat dit niet als aparte afbeelding opgeslagen wordt want dat is niet nodig voor dit voorbeeld. Zoals te zien hierboven heeft de code gewerkt. Ik heb twee kleine aanpassingen gedaan, namelijk: de data voor het inladen stond op een andere plek dus die heb ik even aangepast en ik heb de ggsave uit gecomment zodat deze niet telkens probeert om een afbeelding op te slaan dat je de code runt. Uit het resultaat van de code is te zien dat deze code heel netjes en leesbaar is geschreven en zonder ingrijpende handelingen hetzelfde resultaat geeft als in het paper is laten zien. Ik zou de leesbaarheid van de code op een schaal van 1 (heel slecht) tot 5 (erg goed) een 5 geven aangezien het duidelijk leesbaar is met comments en tussenkopjes. Ik zou de reprodureerbaarheid van dit onderzoek op een schaal van 1 (heel slecht) tot 5 (erg goed) een 5 geven aangezien de code met een paar kleine aanpassingen precies hetzelfde resultaat geeft als gepresenteert in het paper. "],["guerrilla-framework.html", "5 Guerrilla framework", " 5 Guerrilla framework Het guerrilla framework, Ridge (2015) is een manier om data consequent een locatie te geven in een file structure. Het heeft 7 principes, de eerste is: ruimte is goedkoop maar verwarring is duur. Dit houdt in dat het prima is om extra ruimte te gebruiken om bijvoorbeeld readmes toe te voegen omdat het veel sneller werkt. De volgende is: gebruik simpele visuele project structuren. De volgende is automatiseer zo veel mogelijk met code zodat het snel reproduceerbaar is. Verbind data in de analyse omgeving met de werk omgeving. Gebruik version control software zoals git/github. Maak duidelijke afspraken binnen het team over bijvoorbeeld de manier waarop bestanden genoemd worden of wat het principe van de bestanden structuur is. Gebruik code die van begin tot einde loopt dus waar niet nog halverwege handmatig iets aan aangepast hoeft te worden. Om hier een voorbeeld van te geven is hieronder de folder structuur van een voorgaande cursus gegeven: fs::dir_tree(here::here(&quot;assignment_2&quot;, &quot;daur2wouter&quot;)) ## C:/Users/Gebruiker/Documents/hu/datascience/dsfb2/Portfolio/assignment_2/daur2wouter ## +-- assignment_2.html ## +-- assignment_2.Rmd ## +-- citations2.bib ## +-- metagenomics ## | +-- data ## | | +-- bracken ## | | | +-- mock1.bracken ## | | | \\-- README ## | | +-- fastqcMG ## | | | +-- HU1_MOCK1_L001_R1_001_fastqc ## | | | | +-- fastqc.fo ## | | | | +-- fastqc_data.txt ## | | | | +-- fastqc_report.html ## | | | | +-- Icons ## | | | | | +-- error.png ## | | | | | +-- fastqc_icon.png ## | | | | | +-- tick.png ## | | | | | \\-- warning.png ## | | | | +-- Images ## | | | | | +-- adapter_content.png ## | | | | | +-- duplication_levels.png ## | | | | | +-- per_base_n_content.png ## | | | | | +-- per_base_quality1.png ## | | | | | +-- per_base_sequence_content.png ## | | | | | +-- per_sequence_gc_content.png ## | | | | | +-- per_sequence_quality.png ## | | | | | +-- per_tile_quality.png ## | | | | | \\-- sequence_length_distribution.png ## | | | | \\-- summary.txt ## | | | +-- HU1_MOCK1_L001_R1_001_fastqc.html ## | | | +-- HU1_MOCK1_L001_R1_001_fastqc.zip ## | | | +-- HU1_MOCK1_L001_R2_001_fastqc ## | | | | +-- fastqc.fo ## | | | | +-- fastqc_data.txt ## | | | | +-- fastqc_report.html ## | | | | +-- Icons ## | | | | | +-- error.png ## | | | | | +-- fastqc_icon.png ## | | | | | +-- tick.png ## | | | | | \\-- warning.png ## | | | | +-- Images ## | | | | | +-- adapter_content.png ## | | | | | +-- duplication_levels.png ## | | | | | +-- per_base_n_content.png ## | | | | | +-- per_base_quality2.png ## | | | | | +-- per_base_sequence_content.png ## | | | | | +-- per_sequence_gc_content.png ## | | | | | +-- per_sequence_quality.png ## | | | | | +-- per_tile_quality.png ## | | | | | \\-- sequence_length_distribution.png ## | | | | \\-- summary.txt ## | | | +-- HU1_MOCK1_L001_R2_001_fastqc.html ## | | | +-- HU1_MOCK1_L001_R2_001_fastqc.zip ## | | | \\-- README ## | | +-- kraken2 ## | | | +-- mock1 ## | | | | +-- mock1.report ## | | | | +-- mock1_bracken_species.biom ## | | | | \\-- mock1_bracken_species.report ## | | | \\-- README ## | | +-- per_base_quality1.png ## | | +-- per_base_quality2.png ## | | +-- README ## | | \\-- setup_meta_env.yml ## | +-- formatieveMetagenomics.html ## | +-- formatieveMetagenomics.Rmd ## | \\-- README ## +-- README ## \\-- RNAseq ## +-- formatieveRNAseq.html ## +-- formatieveRNAseq.Rmd ## \\-- README "],["dosis-response-analyse.html", "6 Dosis response analyse", " 6 Dosis response analyse Hieronder wordt een dosis respons analyse uitgevoerd. Tijdens het kijken naar de data voor deze analyse valt op dat er wat datapunten missen in rij 192-196 en dat sommige kolommen volledig dezelfde data bevatten wat beter in een metadata bestand gezet had kunnen worden. Als eerste wordt de data ingeladen in R. # inladen data data &lt;- read_excel(here( &quot;assignment_1&quot;, &quot;raw_data&quot;, &quot;CE.LIQ.FLOW.062_Tidydata.xlsx&quot; )) Hierna wordt van de ongefilterde data een scatterplot gemaakt om de data te ontdekken. # maak een scatterplot met de ongefilterde data data %&gt;% ggplot(aes(x= compConcentration, y= RawData, color= compName, shape= expType)) + geom_point() + theme(axis.text.x = element_text(angle =45, hjust = 1)) De data van de x-as is character in plaats van numeriek omdat het als wetenschappelijke notatie is geimporteerd, Dit moet aangepast worden. Hierna kan dezelfde grafiek nog een keer gemaakt worden maar dan met een logaritmische schaal op de x-as. # controleer de unieke waardes van de kolom compConcentration, hierin is te zien dat sommige waardes in wetenschappelijke notatie staan waardoor ze als character worden gelezen unique(data$compConcentration) ## [1] &quot;4.99&quot; &quot;0.499&quot; &quot;4.99E-2&quot; ## [4] &quot;4.9899999999999996E-3&quot; &quot;4.9899999999999999E-4&quot; &quot;4.99E-5&quot; ## [7] &quot;19.5&quot; &quot;1.95&quot; &quot;0.19500000000000001&quot; ## [10] &quot;1.95E-2&quot; &quot;1.9499999999999999E-3&quot; &quot;1.95E-4&quot; ## [13] &quot;0,000195&quot; &quot;1.5&quot; &quot;0&quot; ## [16] &quot;0.5&quot; # verander de waardes met wetenschappelijke notatie naar normale notatie data[&quot;compConcentration&quot;][data[&quot;compConcentration&quot;] == &quot;1.95E-2&quot;] &lt;- &quot;0.0195&quot; data[&quot;compConcentration&quot;][data[&quot;compConcentration&quot;] == &quot;4.9899999999999996E-3&quot;] &lt;- &quot;0.0049899999999999996&quot; data[&quot;compConcentration&quot;][data[&quot;compConcentration&quot;] == &quot;4.9899999999999999E-4&quot;] &lt;- &quot;0.00049899999999999999&quot; data[&quot;compConcentration&quot;][data[&quot;compConcentration&quot;] == &quot;1.9499999999999999E-3&quot;] &lt;- &quot;0.0019499999999999999&quot; data[&quot;compConcentration&quot;][data[&quot;compConcentration&quot;] == &quot;4.99E-2&quot;] &lt;- &quot;0.0499&quot; data[&quot;compConcentration&quot;][data[&quot;compConcentration&quot;] == &quot;4.99E-5&quot;] &lt;- &quot;0.0000499&quot; data[&quot;compConcentration&quot;][data[&quot;compConcentration&quot;] == &quot;1.95E-4&quot;] &lt;- &quot;0.000195&quot; # verander de global options om geen wetenschappelijke notatie in de as.numeric function te hebben options(scipen=999) # controleer de unieke waardes nogmaals, de kolom kan nu omgezet worden naar numeriek unique(data$compConcentration) ## [1] &quot;4.99&quot; &quot;0.499&quot; ## [3] &quot;0.0499&quot; &quot;0.0049899999999999996&quot; ## [5] &quot;0.00049899999999999999&quot; &quot;0.0000499&quot; ## [7] &quot;19.5&quot; &quot;1.95&quot; ## [9] &quot;0.19500000000000001&quot; &quot;0.0195&quot; ## [11] &quot;0.0019499999999999999&quot; &quot;0.000195&quot; ## [13] &quot;0,000195&quot; &quot;1.5&quot; ## [15] &quot;0&quot; &quot;0.5&quot; data$compConcentration &lt;- as.numeric(data$compConcentration) # verander de kolommen &quot;compName&quot; en &quot;expType&quot; naar een factor data$compName &lt;- as.factor(data$compName) data$expType &lt;- as.factor(data$expType) # maak een scatterplot me een beetje jitter zodat de punten niet overlappen data %&gt;% ggplot(aes(x= compConcentration, y= RawData, color= compName, shape= expType)) + geom_jitter(position = position_jitter(0.05)) + scale_x_log10(limits=c(0.01,100)) Allereerst moet deze data nog genormaliseert worden, Dit wordt gedaan door het gemiddelde van de negatieve controle naar 1 te zetten en alle andere waardes een verhouding daarvan te maken. Dit zodat er gekeken kan worden naar verhoudingen en niet naar absolute getallen. # verkrijg het gemiddelde van de controlNegative in de kolom expType norm &lt;- data %&gt;% group_by(expType) %&gt;% summarise_at(vars(RawData), list(name=mean)) # maak een nieuwe kolom met genormaliseerde data norm_data &lt;- data %&gt;% mutate(norm_RawData= RawData / as.numeric(norm[1,2])) # maak de genormaliseerde scatterplot norm_data %&gt;% ggplot(aes(x= compConcentration, y= norm_RawData, color= compName, shape= expType)) + geom_jitter(position = position_jitter(0.05)) + scale_x_log10(limits=c(0.01,100)) De positieve controle voor dit experiment is ethanol en de negatieve controle is S-medium. Als je naar de grafiek kijkt lijkt 2,6-diisopropylnaphtalene langzaam maar zeker af te nemen terwijl naphtalene lang hoog blijft en opeens afvalt en decane juist het omgekeerde doet en snel afvalt en daarna daar blijft hangen. Er lijkt dus wel een correlatie te zijn tussen de concentratie compound en de hoeveelheid getelde nematoden. "],["sql-data-analyse.html", "7 SQL data analyse", " 7 SQL data analyse Hieronder wordt een data analyse uitgevoerd met behulp van SQL. Er zijn drie datasets, de eerste is van de activiteit van de griep in verschillende landen gemeten op verschillende dagen, de tweede is dezelfde informatie maar dan voor de ziekte dengue. Als laatste wordt hier gebruik gemaakt van een bijgevoegde dataset van dslabs genaamd gapminder met algemene demografische informatie per land. Het doel is om de data in te laden in SQL, dat weer aan te vragen met een query en daarna te visualiseren met wat grafieken. Als eerste moet de data ingelezen worden, tidy gemaakt worden en er voor zorgen dat de datum gesplitst wordt over drie kolommen voor de verdere analyse. Daarna wordt het ook verkend om te controleren dat het inlezen goed is gegaan en de kolommen bijvoorbeeld de goede datatypes hebben. # lees de data in fluDf &lt;- read_csv(here(&quot;assignment_7&quot;, &quot;flu_data.csv&quot;), skip = 11) dengueDf &lt;- read_csv(here(&quot;assignment_7&quot;, &quot;dengue.csv&quot;), skip = 11) gapminderDf &lt;- data.frame(gapminder) # maak de griep en dengue data tidy want er zaten meerdere observaties op 1 rij tidyFlu &lt;- fluDf %&gt;% pivot_longer(cols=tail(colnames(fluDf), n= 29), names_to= &quot;country&quot;, values_to= &quot;fluActivity&quot;) tidyDengue &lt;- dengueDf %&gt;% pivot_longer(cols= tail(colnames(dengueDf), n= 10), names_to= &quot;country&quot;, values_to= &quot;dengueActivity&quot;) # maak de datatype voor de country en year kolommen hetzelfde over de verschillende tabellen gapminderDf$country &lt;- as.character(gapminderDf$country) tidyFlu &lt;- tidyFlu %&gt;% separate(col = Date, into = c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;), sep = &quot;-&quot;) tidyDengue &lt;- tidyDengue %&gt;% separate(col = Date, into = c(&quot;year&quot;, &quot;month&quot;, &quot;day&quot;), sep = &quot;-&quot;) # kijk of de datatypes nu overeenkomen en verken verder de data summary(tidyDengue) ## year month day ## Length:6590 Length:6590 Length:6590 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## country dengueActivity ## Length:6590 Min. :0.0000 ## Class :character 1st Qu.:0.0530 ## Mode :character Median :0.1040 ## Mean :0.1390 ## 3rd Qu.:0.1755 ## Max. :1.0000 ## NA&#39;s :327 summary(tidyFlu) ## year month day ## Length:19111 Length:19111 Length:19111 ## Class :character Class :character Class :character ## Mode :character Mode :character Mode :character ## ## ## ## ## country fluActivity ## Length:19111 Min. : 0.0 ## Class :character 1st Qu.: 37.0 ## Mode :character Median : 185.0 ## Mean : 473.7 ## 3rd Qu.: 578.0 ## Max. :10555.0 ## NA&#39;s :1845 summary(gapminderDf) ## country year infant_mortality life_expectancy ## Length:10545 Min. :1960 Min. : 1.50 Min. :13.20 ## Class :character 1st Qu.:1974 1st Qu.: 16.00 1st Qu.:57.50 ## Mode :character Median :1988 Median : 41.50 Median :67.54 ## Mean :1988 Mean : 55.31 Mean :64.81 ## 3rd Qu.:2002 3rd Qu.: 85.10 3rd Qu.:73.00 ## Max. :2016 Max. :276.90 Max. :83.90 ## NA&#39;s :1453 ## fertility population gdp ## Min. :0.840 Min. : 31238 Min. : 40395128 ## 1st Qu.:2.200 1st Qu.: 1333486 1st Qu.: 1845780110 ## Median :3.750 Median : 5009043 Median : 7794215003 ## Mean :4.084 Mean : 27014609 Mean : 147954410013 ## 3rd Qu.:6.000 3rd Qu.: 15231789 3rd Qu.: 55399648248 ## Max. :9.220 Max. :1376048943 Max. :11744219459700 ## NA&#39;s :187 NA&#39;s :185 NA&#39;s :2972 ## continent region ## Africa :2907 Western Asia :1026 ## Americas:2052 Eastern Africa : 912 ## Asia :2679 Western Africa : 912 ## Europe :2223 Caribbean : 741 ## Oceania : 684 South America : 684 ## Southern Europe: 684 ## (Other) :5586 Hierna wordt de gecleande data opgeslagen als CSV en als RDS. # sla de gecleande data op write.csv(tidyFlu, file = here(&quot;assignment_7&quot;, &quot;tidyFlu.csv&quot;), row.names = FALSE) write.csv(tidyDengue, file = here(&quot;assignment_7&quot;, &quot;tidyDengue.csv&quot;)) write.csv(gapminderDf, file = here(&quot;assignment_7&quot;, &quot;gapminderDf.csv&quot;)) write_rds(tidyFlu, file = here(&quot;assignment_7&quot;, &quot;tidyFlu.rds&quot;)) write_rds(tidyDengue, file = here(&quot;assignment_7&quot;, &quot;tidyDengue.rds&quot;)) write_rds(gapminderDf, file = here(&quot;assignment_7&quot;, &quot;gapminderDf.rds&quot;)) Nu wordt er een con object gemaakt wat niet included is in de html zodat men online mijn wachtwoord niet ziet. Daarna worden de CSVs geupload naar de SQL server. # laad de tabellen in de database in dbWriteTable(con, &quot;tidyFlu&quot;, read_csv(here(&quot;assignment_7&quot;, &quot;tidyFlu.csv&quot;)), overwrite = TRUE) dbWriteTable(con, &quot;tidyDengue&quot;, read_csv(here(&quot;assignment_7&quot;, &quot;tidyDengue.csv&quot;)), overwrite = TRUE) dbWriteTable(con, &quot;gapminderDf&quot;, read_csv(here(&quot;assignment_7&quot;, &quot;gapminderDf.csv&quot;)), overwrite = TRUE) Vervolgens wordt er wat verkennende data aangevraagd aan de database om te controleren dat de data goed is over gekomen waarna het uiteindelijk gejoined opgevraagd wordt vanuit de server. # laat de namen van de verschillende tabellen in de database zien dbListTables(con) ## [1] &quot;tidyFlu&quot; &quot;tidyDengue&quot; &quot;gapminderDf&quot; # laat de namen van de kolommen van de verschillende tabellen zien dbListFields(con, dbListTables(con)[1]) ## [1] &quot;year&quot; &quot;month&quot; &quot;day&quot; &quot;country&quot; &quot;fluActivity&quot; dbListFields(con, dbListTables(con)[2]) ## [1] &quot;year&quot; &quot;month&quot; &quot;day&quot; &quot;country&quot; ## [5] &quot;dengueActivity&quot; dbListFields(con, dbListTables(con)[3]) ## [1] &quot;country&quot; &quot;year&quot; &quot;infant_mortality&quot; ## [4] &quot;life_expectancy&quot; &quot;fertility&quot; &quot;population&quot; ## [7] &quot;gdp&quot; &quot;continent&quot; &quot;region&quot; # geef een summary van alle tabellen in de database summary(dbReadTable(con, dbListTables(con)[1])) ## year month day country ## Min. :2002 Length:19111 Length:19111 Length:19111 ## 1st Qu.:2006 Class :character Class :character Class :character ## Median :2009 Mode :character Mode :character Mode :character ## Mean :2009 ## 3rd Qu.:2012 ## Max. :2015 ## ## fluActivity ## Min. : 0.0 ## 1st Qu.: 37.0 ## Median : 185.0 ## Mean : 473.7 ## 3rd Qu.: 578.0 ## Max. :10555.0 ## NA&#39;s :1845 summary(dbReadTable(con, dbListTables(con)[2])) ## year month day country ## Min. :2002 Length:6590 Length:6590 Length:6590 ## 1st Qu.:2006 Class :character Class :character Class :character ## Median :2009 Mode :character Mode :character Mode :character ## Mean :2009 ## 3rd Qu.:2012 ## Max. :2015 ## ## dengueActivity ## Min. :0.0000 ## 1st Qu.:0.0530 ## Median :0.1040 ## Mean :0.1390 ## 3rd Qu.:0.1755 ## Max. :1.0000 ## NA&#39;s :327 summary(dbReadTable(con, dbListTables(con)[3])) ## country year infant_mortality life_expectancy ## Length:10545 Min. :1960 Min. : 1.50 Min. :13.20 ## Class :character 1st Qu.:1974 1st Qu.: 16.00 1st Qu.:57.50 ## Mode :character Median :1988 Median : 41.50 Median :67.54 ## Mean :1988 Mean : 55.31 Mean :64.81 ## 3rd Qu.:2002 3rd Qu.: 85.10 3rd Qu.:73.00 ## Max. :2016 Max. :276.90 Max. :83.90 ## NA&#39;s :1453 ## fertility population gdp ## Min. :0.840 Min. : 31238 Min. : 40395128 ## 1st Qu.:2.200 1st Qu.: 1333486 1st Qu.: 1845780110 ## Median :3.750 Median : 5009043 Median : 7794215003 ## Mean :4.084 Mean : 27014609 Mean : 147954410013 ## 3rd Qu.:6.000 3rd Qu.: 15231789 3rd Qu.: 55399648248 ## Max. :9.220 Max. :1376048943 Max. :11744219459700 ## NA&#39;s :187 NA&#39;s :185 NA&#39;s :2972 ## continent region ## Length:10545 Length:10545 ## Class :character Class :character ## Mode :character Mode :character ## ## ## ## # join de gapminder tabel met de tidyflu tabel joined &lt;- dbGetQuery(con, &quot;SELECT * FROM \\&quot;gapminderDf\\&quot; INNER JOIN \\&quot;tidyFlu\\&quot; ON \\&quot;gapminderDf\\&quot;.year = \\&quot;tidyFlu\\&quot;.year AND \\&quot;gapminderDf\\&quot;.country = \\&quot;tidyFlu\\&quot;.country&quot;) Uiteindelijk wordt de opgevraagde gejoinde data gebruikt om een paar visualisaties van de data mee te doen. Als eerste is er een box plot met op de y-as de levensverwachting in jaren en dit is gegroepeert per regio. De volgende is een lijngrafiek met op de y-as de kindersterfte per duizend kinderen in Argentinië en op de x-as de jaren waarop dit gemeten is. De laatste grafiek is een cirkel diagram met de BBP opgeteld per continent. joined %&gt;% ggplot(aes(color = region, y = life_expectancy)) + geom_boxplot() + labs(title = &quot;Levensverwachting per regio&quot;, y = &quot;Levensverwachting in jaren&quot;, color = &quot;Regio&#39;s&quot;) + theme(axis.title.x=element_blank(), axis.text.x=element_blank(), axis.ticks.x=element_blank()) joined %&gt;% filter(country == &quot;Argentina&quot;) %&gt;% ggplot(aes(x = year, y = infant_mortality)) + geom_line() + scale_x_continuous(breaks = seq(2000,2020,1)) + scale_y_continuous(breaks = seq(10,20,1)) + labs(title = &quot;Kindersterfte per jaar in Argentinië&quot;, x = &quot;Jaren&quot;, y = &quot;Kindersterfte per duizend kinderen&quot;) joined %&gt;% ggplot(aes(x = &quot;&quot;, y = gdp, fill = continent)) + geom_col() + coord_polar(theta = &quot;y&quot;) + theme_void() + labs(title = &quot;Verhouding BBP per continent&quot;, fill = &quot;Continent&quot;) "],["zelfgemaakte-package.html", "8 Zelfgemaakte package", " 8 Zelfgemaakte package Hieronder is een zelfgemaakte package te zien die ook te downloaden en proberen is. Voor de package zie repository: explorers op profile: WouterVerra, zie ook de soccer dataset die bijgevoegd is aan de package. devtools::install_github(&quot;WouterVerra/explorers&quot;, build_vignettes = TRUE, force = TRUE) # laad de package library(explorers) # standaard toegevoegde data soccer # explore functie explore(soccer) ## [[1]] ## [1] &quot;class:&quot; ## ## [[2]] ## [1] &quot;data.frame&quot; ## ## [[3]] ## [1] &quot;columns:&quot; ## ## [[4]] ## [1] &quot;points&quot; &quot;assists&quot; &quot;team&quot; ## ## [[5]] ## [1] &quot;dimensions:&quot; ## ## [[6]] ## [1] 6 3 ## ## [[7]] ## [1] &quot;summary:&quot; ## ## [[8]] ## points assists team ## Min. :2.0 Min. :6.00 Length:6 ## 1st Qu.:3.0 1st Qu.:6.75 Class :character ## Median :3.5 Median :7.00 Mode :character ## Mean :4.0 Mean :7.25 ## 3rd Qu.:4.0 3rd Qu.:7.50 ## Max. :8.0 Max. :9.00 ## NA&#39;s :2 # NAcol functie NAcol(soccer) ## [1] &quot;points = 0&quot; ## [1] &quot;assists = 2&quot; ## [1] &quot;team = 1&quot; # NArow functie NArow(soccer) ## [1] &quot;2 = 1&quot; ## [1] &quot;4 = 2&quot; # replace functie replace(soccer, &quot;points&quot;, 3, 20) De HTML vignette is hieronder te zien, ook is het mogelijk om een uitleg van alle functies op te zoeken door bijvoorbeeld ?replace in de console te typen. "],["parameterizatie-van-rmarkdown-documenten.html", "9 Parameterizatie van Rmarkdown documenten", " 9 Parameterizatie van Rmarkdown documenten Dit is een Rmarkdown bestand met parameters waardoor snel en overzichtelijk verschillende outputs gemaakt kunnen worden. Het gebruik van parameters wordt gepresenteerd met dit voorbeeld van covid 19 data die gedownload is van de ECDC Download COVID-19 Data Sets (2020). Als eerste worden in de YAML header parameters toegevoegd die daarna in de code gecalled kunnen worden waardoor alleen in de YAML header de informatie aangepast hoeft te worden en het overal in het bestand wordt overgenomen. Dit is veel mmakkelijker en met minder kans op menselijke fouten dan meerdere calls in de code aanpassen. Daarna wordt de data ingelezen, meerdere malen gefilterd op verschillende parameters en daarna met ggplot geplot. De eerste plot geeft de hoeveelheid cases van covid 19 in de dagen van de geselecteerde maand in de het geselecteerde jaar in het geselecteerde land. De tweede grafiek geeft de hoeveelheid covid gerelateerde sterfgevallen in de dagen van de geselecteerde maand in de het geselecteerde jaar in het geselecteerde land # read in csv with covid data covid_data &lt;- read.csv(here(&quot;assignment_9&quot;, &quot;data.csv&quot;)) # filter op land covid_data &lt;- covid_data %&gt;% filter(countriesAndTerritories == params$country) # filter op jaar covid_data &lt;- covid_data %&gt;% filter(year == params$year) # filter op maand covid_data &lt;- covid_data %&gt;% filter(month == params$month) # plot the cases covid_data %&gt;% ggplot(aes(x= day, y= cases)) + geom_line() + geom_point() + labs(title= paste0(&quot;Covid 19 cases in: &quot;, params$country, &quot; in: &quot;, params$month, &quot;/&quot;, params$year), y= &quot;Covid cases&quot;) # plot the deaths covid_data %&gt;% ggplot(aes(x= day, y= deaths)) + geom_line() + geom_point() + labs(title= paste0(&quot;Covid 19 deaths in: &quot;, params$country, &quot; in: &quot;, params$month, &quot;/&quot;, params$year), y= &quot;Covid deaths&quot;) "],["computer-vision-morris-watermaze.html", "10 Computer vision morris watermaze", " 10 Computer vision morris watermaze Hieronder heb ik een computer vision blob detectie analyse gedaan op morris watermaze data met behulp van opencv met python als codebase. Het morris watermaze is een grote bak met wit gekleurt water met daarin een klein tableau net onder de waterlijn waardoor het niet goed zichtbaar is. Hier worden over een paar dagen tijd ratten geleerd dat er een tableau is onder het water waar ze op kunnen staan en dus niet hoeven te zwemmen, dat vinden ze fijn en willen ze dus zo snel mogelijk bereiken. de manier waarop ze zoeken naar het tableau is interresant voor de onderzoekers. De analyse werd voorheen gedaan door de video af te spelen op een beeldscherm en daar dan een papier overheen te houden en dan de route van de rat overtekenen terwijl dit gebeurt. Dit is natuurlijk niet heel erg handig en kan sneller. Het doel van dit script is om ratten die in het water zitten automatisch te volgen met computer vision. Hiervoor wordt als eerste de video ingeladen waarna er een witte cirkel om de waterbak heen getekent wordt zodat de achtergrond niet interfereerd met de blob detectie OpenCV Python Tutorial #4 - Drawing (Lines, Images, Circles &amp; Text) (n.d.). Daarna moet de video van BGR naar HSV omgezet worden omdat de blob detectie niet werkt op bgr format OpenCV Python Tutorial #5 - Colors and Color Detection (n.d.). Daarna worden de uiterste kleuropties bepaald voor de mask die bepaald gaat worden HTML Color Picker (n.d.). De mask geeft een binaire video waar als de mask overeenkomt daar een 1 staat en het anders een 0 wordt OpenCV Python Tutorial #5 - Colors and Color Detection (n.d.). Hierna wordt er een blobdetectie op uitgevoerd waarbij het centrum van de blob gevonden wordt How to Detect Colors in OpenCV [Python] (n.d.) waarna hier een cirkel omheen getekent kan worden OpenCV: Contour Features (n.d.). Hierna wordt deze aangepaste video opgeslagen als .avi bestand Capture and Save Video Using OpenCV Python (n.d.). Dit kan dan het gemakkelijkst met VLC mediaplayer omgezet worden naar een .mp4 bestand How to Convert AVI to Mp4 Using VLC Media Player (n.d.) wat daarna embed kan worden in deze markdown Embed_video: Embed Video in R Markdown Documents in Mccarthy-m-g/Embedr: Embed Multimedia Files in HTML Documents (n.d.). Het volgende doel van dit experiment zou zijn om in plaats van een verplaatsende cirkel een blijvende lijn te tekenen zodat dit in 1 keer opgeslagen zou kunnen worden als .jpeg. import cv2 import numpy as np cap = cv2.VideoCapture(&#39;0115_LH39_d1_t4.mp4&#39;) # laad de video in out = cv2.VideoWriter(&#39;output.avi&#39;, cv2.VideoWriter_fourcc(*&#39;MJPG&#39;), 30.0, (int(cap.get(3)), int(cap.get(4)))) while True: # leest de afbeelding van de video ret, frame = cap.read() # teken een cirkel zodat de achtergrond niet wordt meegenomen in de mask img = cv2.circle(frame, (640,480), 660, (255,255,255), 400) # zet afbeelding om van bgr naar hsv zodat opencv er mee kan werken hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV) # bepaal de uiterste kleuren voor de mask dark_black = np.array([0, 0, 0]) light_black = np.array([0, 0, 60]) # de mask bepaalt welke kleuren in range zijn en maakt hier een binaire afbeelding van mask = cv2.inRange(hsv, dark_black, light_black) result = cv2.bitwise_and(frame, frame, mask=mask) # vind de contouren contours, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) if len(contours) !=0: for contour in contours: if cv2.contourArea(contour) &gt; 500: (x, y), radius = cv2.minEnclosingCircle(contour) center = (int(x), int(y)) radius = int(radius) cv2.circle(frame, center, radius, (0,255,0), 4) # sla de video op out.write(frame) # laat de video zien in een popup als watermaze result cv2.imshow(&#39;watermaze result&#39;, frame) # als je op q drukt sluit de loop vroegtijdig af if cv2.waitKey(1) == ord(&#39;q&#39;): break cap.release() out.release() cv2.destroyAllWindows() Hieronder is de input file te zien. Your browser does not support the video tag; for browser support, please see: https://www.w3schools.com/tags/tag_video.asp Hier is de output van de bovenstaande python code te zien. Your browser does not support the video tag; for browser support, please see: https://www.w3schools.com/tags/tag_video.asp "],["bibliografie.html", "11 Bibliografie", " 11 Bibliografie "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
